{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j9Yvb5ViDtJa"
   },
   "outputs": [],
   "source": [
    "# Installation of nltk\n",
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbe6lB1UDtJd"
   },
   "source": [
    "### Text Preprocessing\n",
    "Following code can be used for text preprocessing useful for various NLP applications.\n",
    "\n",
    "First we need to import nltk\n",
    "\n",
    "For a given text, we can do sentence tokenization and word tokenization using nltk library functions.\n",
    "We can remove the punctuations using string library.\n",
    "\n",
    "We can then remove stop words in English to get the important words in the text.\n",
    "\n",
    "We also perform stemming and lemmatization. Stemming and Lemmatization are two different techniques that help reduce our data space. We donâ€™t need to check every single form of a word for reducing the size of the big data corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RAuvlGtBDtJe",
    "outputId": "47c1b9b5-f41c-449d-d7a2-896bce35d78b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vysakh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/vysakh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/vysakh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/vysakh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vysakh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk library for using its different functions\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "x3AE3Dj5DtJf"
   },
   "outputs": [],
   "source": [
    "#  Sentence Tokenization  - Tokenizes sentences from text\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5HOsv8a1DtJf"
   },
   "outputs": [],
   "source": [
    "# Word Tokenization  - Tokenizes words in sentences\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Dh8sZ5ZtDtJf"
   },
   "outputs": [],
   "source": [
    "statement = \"Microsoft is trying to buy France based startup at $7 Million. The quick brown fox, snatch the piece of cube from mouth of black crow.Tesla to build solar electric startup in gujrat for $70 million\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7L51l3fJDtJf",
    "outputId": "0bf5a316-4edd-4e6f-cf26-b8c6d1e2d25b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower: microsoft is trying to buy france based startup at $7 million. the quick brown fox, snatch the piece of cube from mouth of black crow.tesla to build solar electric startup in gujrat for $70 million\n"
     ]
    }
   ],
   "source": [
    "statement=statement.lower()\n",
    "print(\"Lower: \" + statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wdsOjSqoDtJg",
    "outputId": "c34743c7-445e-4dc5-a834-54242af02abd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['microsoft is trying to buy france based startup at $7 million.', 'the quick brown fox, snatch the piece of cube from mouth of black crow.tesla to build solar electric startup in gujrat for $70 million']\n",
      "['microsoft', 'is', 'trying', 'to', 'buy', 'france', 'based', 'startup', 'at', '$', '7', 'million', '.', 'the', 'quick', 'brown', 'fox', ',', 'snatch', 'the', 'piece', 'of', 'cube', 'from', 'mouth', 'of', 'black', 'crow.tesla', 'to', 'build', 'solar', 'electric', 'startup', 'in', 'gujrat', 'for', '$', '70', 'million']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(statement)\n",
    "print(sentences)\n",
    "words = word_tokenize(statement)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1eWHFoHDDtJg",
    "outputId": "64ed7cbc-4883-428e-a904-62e76be50787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft is trying to buy france based startup at $7 million.\n",
      "the quick brown fox, snatch the piece of cube from mouth of black crow.tesla to build solar electric startup in gujrat for $70 million\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hRoqjJAhDtJg",
    "outputId": "65ef46aa-5a1c-422c-f2c8-2529024cb311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft\n",
      "is\n",
      "trying\n",
      "to\n",
      "buy\n",
      "france\n",
      "based\n",
      "startup\n",
      "at\n",
      "7\n",
      "million\n",
      "the\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "snatch\n",
      "the\n",
      "piece\n",
      "of\n",
      "cube\n",
      "from\n",
      "mouth\n",
      "of\n",
      "black\n",
      "crow.tesla\n",
      "to\n",
      "build\n",
      "solar\n",
      "electric\n",
      "startup\n",
      "in\n",
      "gujrat\n",
      "for\n",
      "70\n",
      "million\n"
     ]
    }
   ],
   "source": [
    " # Remove punctuations\n",
    "for word in words:\n",
    "    if word not in string.punctuation:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UKdoJFqNDtJg",
    "outputId": "b4535e41-d34e-4551-a4eb-13a7ce8bd207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['microsoft', 'is', 'trying', 'to', 'buy', 'france', 'based', 'startup', 'at', '7', 'million', 'the', 'quick', 'brown', 'fox', 'snatch', 'the', 'piece', 'of', 'cube', 'from', 'mouth', 'of', 'black', 'crow.tesla', 'to', 'build', 'solar', 'electric', 'startup', 'in', 'gujrat', 'for', '70', 'million']\n"
     ]
    }
   ],
   "source": [
    "only_words=[w for w in words if not w in string.punctuation]\n",
    "print(only_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HgjmUHODtJh",
    "outputId": "cfa5194c-3f97-482a-d0d9-60ca3c2b494e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft:1\n",
      "is:1\n",
      "trying:1\n",
      "to:2\n",
      "buy:1\n",
      "france:1\n",
      "based:1\n",
      "startup:2\n",
      "at:1\n",
      "7:1\n",
      "million:2\n",
      "the:2\n",
      "quick:1\n",
      "brown:1\n",
      "fox:1\n",
      "snatch:1\n",
      "piece:1\n",
      "of:2\n",
      "cube:1\n",
      "from:1\n",
      "mouth:1\n",
      "black:1\n",
      "crow.tesla:1\n",
      "build:1\n",
      "solar:1\n",
      "electric:1\n",
      "in:1\n",
      "gujrat:1\n",
      "for:1\n",
      "70:1\n"
     ]
    }
   ],
   "source": [
    "#  Count Word Frequency\n",
    "\n",
    "freq = nltk.FreqDist(only_words)\n",
    "for key, val in freq.items():\n",
    "    print(str(key) + ':' + str(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f9SXeZKODtJh"
   },
   "outputs": [],
   "source": [
    "#Removal of stop words from the text\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Awsqy-KhDtJh",
    "outputId": "bce5e346-9645-45fd-f6cf-688f6fe0d38e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"that'll\", 'couldn', \"shouldn't\", 'which', 'she', 't', 'just', 'a', \"doesn't\", \"aren't\", 'for', 'hers', 'him', 'an', \"mightn't\", 'does', 'only', 'am', 'from', 'further', 'can', 'will', 'haven', 'who', \"haven't\", \"mustn't\", \"it's\", 'be', 'again', 'all', \"should've\", 'o', 'they', 'above', 'most', 'very', \"didn't\", 'shouldn', 'yourself', 'don', 'ma', 'having', 'was', 'whom', 'herself', 'his', 'isn', 'me', 'because', 'them', 'i', \"hadn't\", 'is', 'too', 'did', 'than', 'yours', 'same', 'himself', 'their', 'nor', 'once', \"don't\", 'as', 'these', 're', \"she's\", 'her', \"you'd\", 's', 'against', 'its', 'each', 'out', 'until', 'but', 'before', 'the', 'below', 'theirs', 'when', 'over', 'should', 'here', 'at', 'into', 'some', 'ours', 'been', 'didn', 'll', \"shan't\", 'being', 'and', \"hasn't\", 'by', 'few', 'how', 'other', 'hadn', 'or', 'now', 'he', 'no', 'while', \"you've\", 'we', 'any', 'had', 've', 'ain', \"couldn't\", 'doing', 'aren', \"needn't\", 'it', 'then', 'your', \"you're\", \"weren't\", 'itself', 'about', 'under', 'd', 'wasn', 'yourselves', 'of', \"you'll\", 'if', 'so', 'there', 'won', 'with', 'you', 'those', 'ourselves', 'm', 'that', 'to', 'mustn', 'are', 'doesn', 'needn', \"wouldn't\", 'were', 'up', 'my', 'own', 'why', 'hasn', 'y', 'after', 'where', 'do', 'have', 'what', 'through', 'weren', 'wouldn', 'during', 'this', 'such', \"wasn't\", 'more', 'our', 'mightn', \"won't\", 'shan', 'on', 'has', 'not', 'themselves', 'down', 'in', \"isn't\", 'off', 'both', 'between', 'myself'}\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "# List of English stop words\n",
    "english_stop_words=set(stopwords.words(\"english\"))\n",
    "print(english_stop_words)\n",
    "print(len(english_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRfnAuZcDtJh",
    "outputId": "b29d6e9d-620d-4e58-cffb-5f87379ef3cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['microsoft', 'trying', 'buy', 'france', 'based', 'startup', '7', 'million', 'quick', 'brown', 'fox', 'snatch', 'piece', 'cube', 'mouth', 'black', 'crow.tesla', 'build', 'solar', 'electric', 'startup', 'gujrat', '70', 'million']\n"
     ]
    }
   ],
   "source": [
    "# Removal of stop words from the text\n",
    "keywords=[w for w in only_words if not w in english_stop_words]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xERwpJFDDtJh"
   },
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization in NLP is the process through which several different forms of the same word are mapped to one single form, which we can call the root form or the base form. In more technical terms, the root form is called a lemma. By reducing the number of forms a word can take, we make sure that we reduce our data space and that we donâ€™t have to check every single form of a word. It helps us ignore morphological variations on a single word. Lemmatization brings context to the words.So it goes a steps further by linking words with similar meaning to one word. For example if a paragraph has words like cars, trains and automobile, then it will link all of them to automobile. In the below program we use the WordNet lexical database for lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dc0b_76dDtJh",
    "outputId": "677bda4a-673d-4bdb-ca71-9807d63664ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['microsoft', 'trying', 'buy', 'france', 'based', 'startup', '7', 'million', 'quick', 'brown', 'fox', 'snatch', 'piece', 'cube', 'mouth', 'black', 'crow.tesla', 'build', 'solar', 'electric', 'startup', 'gujrat', '70', 'million']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "keywords=[w for w in keywords if w in wordnet_lemmatizer.lemmatize(w)]\n",
    "print(keywords)\n",
    "#Next find the roots of the word\n",
    "#for w in keywords:\n",
    " #   lemmatized_words=wordnet_lemmatizer.lemmatize(w)\n",
    "  #  print(wordnet_lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tx_zcQW_DtJi"
   },
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming in NLP is the process of removing prefixes and suffixes from words so that they are reduced to simpler forms which are called stems. The purpose of stemming is to reduce our vocabulary and dimensionality for NLP tasks and to improve speed and efficiency in information retrieval and information processing tasks. Stemming is a simpler, faster process than lemmatization. The difference is that stemming is usually only rule-based approach. And, as we've showed with our earlier example, rule-based approaches can fail very quickly on more complex examples. But for most problems, it works well enough. Many search engines use stemming to improve their search results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SGiOF6OKDtJi"
   },
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lP_GuV9iDtJi",
    "outputId": "03e6bf0b-fcca-47f6-8c2c-6e2e7938425d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft\n",
      "tri\n",
      "buy\n",
      "franc\n",
      "base\n",
      "startup\n",
      "7\n",
      "million\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "snatch\n",
      "piec\n",
      "cube\n",
      "mouth\n",
      "black\n",
      "crow.tesla\n",
      "build\n",
      "solar\n",
      "electr\n",
      "startup\n",
      "gujrat\n",
      "70\n",
      "million\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "#Next find the roots of the word\n",
    "for w in keywords:\n",
    "      print(porter_stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "p_nQCTT_DtJj"
   },
   "outputs": [],
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gw-PNL9xDtJj",
    "outputId": "dfde2c64-e840-41dd-8919-52b535d1441f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('microsoft', 'JJ'), ('trying', 'VBG'), ('buy', 'NN'), ('france', 'NN'), ('based', 'VBN'), ('startup', '$'), ('7', 'CD'), ('million', 'CD'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('snatch', 'VBP'), ('piece', 'NN'), ('cube', 'NN'), ('mouth', 'JJ'), ('black', 'JJ'), ('crow.tesla', 'NN'), ('build', 'VB'), ('solar', 'JJ'), ('electric', 'JJ'), ('startup', 'NN'), ('gujrat', '$'), ('70', 'CD'), ('million', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation of spacy\n",
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "eXn_wbtkDtJj"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "npl = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IX0tqJeoDtJj",
    "outputId": "81adac6c-ff76-48a1-c944-9548352a279b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple --> Apple , PROPN , NNP , nsubj , False --> noun, proper singular\n",
      "is --> be , AUX , VBZ , aux , True --> verb, 3rd person singular present\n",
      "looking --> look , VERB , VBG , ROOT , False --> verb, gerund or present participle\n",
      "at --> at , ADP , IN , prep , True --> conjunction, subordinating or preposition\n",
      "buying --> buy , VERB , VBG , pcomp , False --> verb, gerund or present participle\n",
      "U.K. --> U.K. , PROPN , NNP , dobj , False --> noun, proper singular\n",
      "startup --> startup , NOUN , NN , dep , False --> noun, singular or mass\n",
      "for --> for , ADP , IN , prep , True --> conjunction, subordinating or preposition\n",
      "$ --> $ , SYM , $ , quantmod , False --> symbol, currency\n",
      "1 --> 1 , NUM , CD , compound , False --> cardinal number\n",
      "billion --> billion , NUM , CD , pobj , False --> cardinal number\n"
     ]
    }
   ],
   "source": [
    "doc1 = npl(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text,\"-->\", token.lemma_, \",\",token.pos_,\",\", token.tag_,\",\", token.dep_,\",\",\n",
    "          token.is_stop,\"-->\", spacy.explain(token.tag_))\n",
    "#SpaCy does not provide a built-in function for Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YyLU4uVzDtJk",
    "outputId": "67e59b38-0927-4602-fca5-98cd88d8b999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[startup] \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "nouns = []\n",
    "adjectives = []\n",
    "for token in doc1:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "print(nouns,\"\\n\",adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "Wogh1d3jDtJk",
    "outputId": "e1a6e19e-ce6b-45c5-d671-2ffe78e490ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"70447ddf019b4a5bba6dba32bd9858bc-0\" class=\"displacy\" width=\"1150\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1050\">billion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1050\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70447ddf019b4a5bba6dba32bd9858bc-0-0\" stroke-width=\"2px\" d=\"M70,152.0 C70,52.0 245.0,52.0 245.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70447ddf019b4a5bba6dba32bd9858bc-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,154.0 L62,142.0 78,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70447ddf019b4a5bba6dba32bd9858bc-0-1\" stroke-width=\"2px\" d=\"M170,152.0 C170,102.0 240.0,102.0 240.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70447ddf019b4a5bba6dba32bd9858bc-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M170,154.0 L162,142.0 178,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70447ddf019b4a5bba6dba32bd9858bc-0-2\" stroke-width=\"2px\" d=\"M270,152.0 C270,102.0 340.0,102.0 340.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70447ddf019b4a5bba6dba32bd9858bc-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M340.0,154.0 L348.0,142.0 332.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70447ddf019b4a5bba6dba32bd9858bc-0-3\" stroke-width=\"2px\" d=\"M370,152.0 C370,102.0 440.0,102.0 440.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70447ddf019b4a5bba6dba32bd9858bc-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M440.0,154.0 L448.0,142.0 432.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70447ddf019b4a5bba6dba32bd9858bc-0-4\" stroke-width=\"2px\" d=\"M470,152.0 C470,102.0 540.0,102.0 540.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70447ddf019b4a5bba6dba32bd9858bc-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M540.0,154.0 L548.0,142.0 532.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70447ddf019b4a5bba6dba32bd9858bc-0-5\" stroke-width=\"2px\" d=\"M270,152.0 C270,52.0 645.0,52.0 645.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70447ddf019b4a5bba6dba32bd9858bc-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M645.0,154.0 L653.0,142.0 637.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70447ddf019b4a5bba6dba32bd9858bc-0-6\" stroke-width=\"2px\" d=\"M670,152.0 C670,102.0 740.0,102.0 740.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70447ddf019b4a5bba6dba32bd9858bc-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,154.0 L748.0,142.0 732.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70447ddf019b4a5bba6dba32bd9858bc-0-7\" stroke-width=\"2px\" d=\"M870,152.0 C870,52.0 1045.0,52.0 1045.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70447ddf019b4a5bba6dba32bd9858bc-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M870,154.0 L862,142.0 878,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70447ddf019b4a5bba6dba32bd9858bc-0-8\" stroke-width=\"2px\" d=\"M970,152.0 C970,102.0 1040.0,102.0 1040.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70447ddf019b4a5bba6dba32bd9858bc-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M970,154.0 L962,142.0 978,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70447ddf019b4a5bba6dba32bd9858bc-0-9\" stroke-width=\"2px\" d=\"M770,152.0 C770,2.0 1050.0,2.0 1050.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70447ddf019b4a5bba6dba32bd9858bc-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1050.0,154.0 L1058.0,142.0 1042.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc1, style='dep', jupyter=True, options = {'distance':100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rHd4S4_XDtJk",
    "outputId": "b2774828-953d-4e62-b47f-b92cb9138bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple : ORG : Companies, agencies, institutions, etc.\n",
      "U.K. : GPE : Countries, cities, states\n",
      "$1 billion : MONEY : Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for entity in doc1.ents:\n",
    "  print(entity,\":\", entity.label_, \":\",spacy.explain(entity.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "xyRkx-Z-DtJk",
    "outputId": "6a2d3c4b-0497-47f3-c903-08078137ab64"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc1, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da_rGUwODtJk",
    "outputId": "5a47fb3e-2246-4611-9b60-1672cf91cc6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'she', 'a', 'well', 'sometimes', 'therefore', 'â€™m', 'seeming', 'from', 'fifty', 'who', 'often', 'all', 'be', 'nine', 'above', 'show', 'even', 'became', 'herself', 'among', 'upon', 'next', \"'d\", 'did', 'thereupon', 'yours', 'twenty', 'their', 'elsewhere', 'her', 'neither', 'bottom', 'but', 'hundred', 'almost', 'move', 'below', 'back', 'along', 'should', 'into', 'full', 'many', 'being', 'by', 'other', 'never', 'whither', 'now', 'becomes', 'he', 'unless', 'while', 'any', 'say', 'anyone', 'side', 'last', 'your', 'itself', 'about', 'might', 'moreover', 'â€˜re', 'if', 'there', 'afterwards', 'keep', 'seem', 'that', 'are', 'my', 'have', 'always', 'this', 'whereby', 'less', 'anyhow', \"n't\", 'done', 'in', 'three', 'becoming', 'perhaps', 'nâ€˜t', 'whether', 'hers', 'an', 'beyond', 'only', 'am', 'together', 'amongst', 'throughout', 'cannot', 'again', 'front', 'thence', 'somehow', 'yourself', 'anything', 'whom', 'may', 'his', 'six', 'call', 'either', 'is', 'within', 'forty', 'though', 'whenever', 'as', 'these', 'least', 'seemed', 'formerly', 'yet', 'something', 'latter', 'before', 'us', 'the', 'fifteen', 'whatever', 'when', 'here', 'been', 'twelve', 'â€™d', 'whereupon', 'how', 'eight', 'â€™s', 'we', 'anywhere', 'doing', 'amount', 'please', 'under', 'you', 'per', 'ourselves', 'thereby', 'were', 'towards', 'own', 'why', 'onto', 'various', 'â€˜ve', 'beforehand', 'do', 'through', 'several', 'more', 'our', 'has', 'themselves', 'off', 'third', 'between', 'anyway', 'latterly', 'another', 'must', 'see', 'using', 'eleven', 'him', 'name', 'every', 'does', 'really', 'further', 'will', 'can', 'made', 'they', 'thereafter', 'hereafter', 'take', 'become', 'whoever', 'was', \"'m\", 'everything', 'â€˜ll', 'because', 'me', 'them', 'i', 'mine', 'indeed', 'give', 'nothing', 'otherwise', 'than', 'himself', \"'re\", 'once', 're', 'due', 'much', 'against', 'each', 'until', 'others', 'two', 'beside', 'first', 'get', 'also', 'at', 'â€˜m', 'someone', 'regarding', 'already', 'noone', 'few', 'could', 'no', \"'s\", 'quite', 'it', 'nâ€™t', 'yourselves', 'somewhere', 'â€™re', 'so', 'nevertheless', 'one', 'four', 'those', 'five', 'rather', 'wherein', 'wherever', 'hereby', 'thru', 'after', 'everyone', 'what', 'around', 'everywhere', 'on', 'â€˜d', 'not', 'ca', 'empty', 'most', 'â€™ve', 'however', 'which', 'thus', 'just', 'for', \"'ve\", 'part', 'although', 'except', 'alone', 'go', 'namely', 'very', \"'ll\", 'hence', 'without', 'would', 'via', 'too', 'seems', 'whereas', 'same', 'still', 'nor', 'besides', 'top', 'its', 'out', 'over', 'ten', 'herein', 'some', 'ours', 'across', 'and', 'nowhere', 'enough', 'or', 'used', 'nobody', 'mostly', 'had', 'ever', 'then', 'hereupon', 'toward', 'sixty', 'of', 'whose', 'none', 'with', 'behind', 'make', 'since', 'whereafter', 'to', 'up', 'â€™ll', 'therein', 'else', 'where', 'sometime', 'whence', 'put', 'during', 'serious', 'such', 'down', 'whole', 'former', 'meanwhile', 'both', 'â€˜s', 'myself'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(spacy_stopwords)\n",
    "len(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bepFgoaDtJk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
